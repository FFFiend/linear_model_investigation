---
title: "Investigating the correlation between a vote for Biden or Trump based on a persons education, gender, race and age."
author: 
  - Fares Alkorani
  - Owais Zahid
thanks: "Code and data are available at: https://github.com/FFFiend/linear_model_investigation."
date: today
date-format: long
abstract: "In this paper, we discuss the correlation between various statistics of the respondents to the CES2020 election survey, and whether they voted for Trump or Biden. The statistics are: age, education, gender and race."
format: pdf
number-sections: true
bibliography: references.bib
---
# Introduction

We are concerned with assessing whether we can predict what candidate a new survey respondent is going to vote for, given their age, education, gender and race. Accordingly, we shall be simulating a similar scenario for the 2020 US presidential election below. 

# Data {#sec-data}

We will be using the 2020 Cooperative Election Study (CES) (CITATION TODO) as our dataset, which can be previewed below:

```{r}
#| include: true
#| warning: false
#| message: false
#| echo: false

library(dplyr)
library(knitr)
library(tidyverse)

raw_data = read.csv("~/linear_model_investigation/data/ces2020_raw.parquet")
cat("Table 1. Header for raw election survey data")
kable(head(raw_data))
```
To elaborate further on the dataset starting with the ```votereg``` column, values 1 and 2 signifies whether a person has voted or not, respectively. Naturally, for entries with a value of 2 under this column, the corresponding ```CC20_410``` value is NA, and in the other case the values 1 and 2 signify a vote for Biden or Trump respectively. The naming of this column corresponds to the question in the survey from which the data for it was collected, which asks "For whom did you vote for President of the United States?" (citation needed), with a total of 6 options, where Biden and Trump correspond to the first two.

Next, there are two values for gender: 1 and 2 corresponding to Male and Female. There are also a total of 8 categories under race corresponding to numbers 1 through 8, as well as 6 education categories ranging from "No HS" to "Post-grad".

Since we are interested with ages and not respondent birth years, we may process the dataset as such to reflect this:

```{r}
#| include: true
#| warning: false
#| message: false
#| echo: false

raw_data$birthyr <- as.numeric(raw_data$birthyr)
current_year <- 2020

# Subtract current year from birth year column to get age
raw_data <- raw_data %>% mutate(birthyr = current_year - birthyr)
# rename to age
names(raw_data)[names(raw_data) == "birthyr"] <- "age"
cat("Table 2. Header for 2020 US Election Survey data with the birthyr column converted to ages")
kable(head(raw_data))

# save data back into the csv.
write.csv(raw_data,"~/linear_model_investigation/data/ces2020.parquet")
```
We shall now filter for non NA values within the ```CC20_410``` column, as we are only concerned with respondents that cast a vote, which also means we filter for entries with a ```votereg``` value of 1. Additionally, we shall map each education level and race value to its corresponding value from the survey, and we finally obtain a table as follows:

```{r}
#| include: true
#| warning: false
#| message: false
#| echo: false
# load in libraries.
library(tidyverse)
library(dplyr)
library(rstanarm)
library(modelsummary)

# read and convert values in each column to integer.
ces2020 <-
  read_csv(
    "~/linear_model_investigation/data/ces2020.parquet",
    col_types =
      cols(
        "votereg" = col_integer(),
        "CC20_410" = col_integer(),
        "gender" = col_integer(),
        "educ" = col_integer(),
        "race" = col_integer(),
        "age" = col_integer()
      )
  )

# formatting data, mapping race ids to races.
ces2020 <-
  ces2020 |>
  filter(votereg == 1,
         CC20_410 %in% c(1, 2)) |>
  mutate(
    voted_for = if_else(CC20_410 == 1, "Biden", "Trump"),
    voted_for = as_factor(voted_for),
    gender = if_else(gender == 1, "Male", "Female"),
    education = case_when(
      educ == 1 ~ "No HS",
      educ == 2 ~ "High school graduate",
      educ == 3 ~ "Some college",
      educ == 4 ~ "2-year",
      educ == 5 ~ "4-year",
      educ == 6 ~ "Post-grad"
    ),
    education = factor(
      education,
      levels = c(
        "No HS",
        "High school graduate",
        "Some college",
        "2-year",
        "4-year",
        "Post-grad"
      )
    ),
    race = case_when(
      race == 1 ~ "White",
      race == 2 ~ "Black",
      race == 3 ~ "Hispanic",
      race == 4 ~ "Asian",
      race == 5 ~ "Native American",
      race == 6 ~ "Middle Eastern",
      race == 7 ~ "Two or more races",
      race == 8 ~ "Other"
    ),
    race = factor(
      race,
      levels = c(
        "White",
        "Black",
        "Hispanic",
        "Asian",
        "Native American",
        "Middle Eastern",
        "Two or more races",
        "Other"
      )
    )
  ) |> select(voted_for, gender, education, race, age)
cat("Table 3. Header for Processed 2020 US Election Survey data.")
# preview data after mutation.
kable(head(ces2020))
```
TODO: complete bar chart section from the worked example.

# Model
We resort to using a Logistic Regression model since we are interested in predicting the candidate a survey respondent voted for, given the former's age, gender, education and race. A logistic regression model is suitable as our response variables (1 for Biden and 0 for Trump) are clearly binary. 

## Model set-up
\begin{align} 
y_i|\pi_i, &\sim \mbox{Bern}(\pi_i) \\
logit(\pi_i) = \beta_0 + \beta_1 \times gender_i + \beta_2 \times age_i + \beta_3 \times education_i + \beta_4 \times race_i \\ 
\beta_0 &\sim \mbox{Normal}(0, 2.5) \\
\beta_1 &\sim \mbox{Normal}(0, 2.5) \\
\beta_2 &\sim \mbox{Normal}(0, 2.5) \\
\beta_3 &\sim \mbox{Normal}(0, 2.5) \\
\beta_4 &\sim \mbox{Normal}(0, 2.5)
\end{align}

We define the response variable $y_i$ to be the candidate that the $ith$ respondent voted for. Note that the value of $y_i$ is either 0 or 1 indicating whether the candidate in question voted for Trump or Biden respectively, and that $y_i$ is conditioned on $\pi_i$ and follows a Bernoulli distribution with parameter $\pi_i$

The explanatory variables $gender_i$, $age_i$, $race_i$ and $education_i$ are the selected attributes (with the same names, of) of the $ith$ respondent. 

We have defined normally distributed priors for the y-intercept $\beta_0$ as well as each of the coefficients of the explanatory variables $\beta_1$, $\beta_2$, $\beta_3$ & $\beta_4$ each with a mean of 0 and standard deviation of 2.5. 

Note that the crux of our model, i.e $logit(\pi_i)$ is expressed as a linear combination of our explanatory variables along with the y-intercept $\beta_0$

Thus, the model enables us to estimate whether a candidate voted for Trump or Biden based on their education, gender, race and age. The logistic nature of the model ensures that any outputs or response variables obtained are either 0 or 1. 

We run the model in R [@citeR] using the `rstanarm` package of @rstanarm. We use the default priors from `rstanarm`.


### Model justification
**Binary Outcome**: logistic regression is specifically designed for binary outcomes, which is suitable for predicting whether a survey respondent voted for Biden (1) or Trump (0). Logistic regression models the probability of the response variable falling into a particular category given the predictor variables.

**Interpretability**: Logistic regression provides easily interpretable results. The coefficients associated with each predictor variable represent the change in the log-odds of the outcome for a one-unit change in the predictor variable. This allows for clear interpretation of the effects of each predictor on the likelihood of voting for a particular candidate.

**Efficiency**: Logistic regression tends to perform well even with relatively small sample sizes compared to more complex models. Given that we have a limited set of predictor variables (age, gender, education, and race), logistic regression can efficiently model the relationship between these variables and the voting outcome.

**Regularization Techniques**: If necessary, logistic regression can be extended with regularization techniques such as L1 (Lasso) or L2 (Ridge) regularization to prevent overfitting and improve generalization performance, especially if there are many predictor variables or multicollinearity issues, which can be a useful strategy if overfitting has been observed upon analyzing the results of the model.

**Statistical Inference**: Logistic regression provides inferential statistics such as p-values and confidence intervals for the estimated coefficients, allowing you to assess the significance of each predictor variable in predicting the outcome.

Overall, logistic regression is a robust and interpretable method for modeling binary outcomes like voting preferences, making it a suitable choice for us to indicate what presidential candidate a survey respondent voted for.

# Results

Our results are summarized in the table  below:
&nbsp;
```{r}
#| include: true
#| warning: false
#| message: false
#| echo: false
political_preferences <-
  readRDS(file = "~/linear_model_investigation/data/political_preferences.rds")

# model summary
cat("Table 4. Whether a respondent is likely to vote for Biden based on their gender, education, race and age (n=8000).")
modelsummary::modelsummary(
  list(
    "Support Biden" = political_preferences
  ),
  statistic = "mad"
)
```

**Gender**: Test

**Education**: Test

**Race**: Test

**Age**:  Test

# Discussion

## First discussion point {#sec-first-point}

If my paper were 10 pages, then should be be at least 2.5 pages. The discussion is a chance to show off what you know and what you learnt from all this. 

## Second discussion point

## Third discussion point

## Weaknesses and next steps

Weaknesses and next steps should also be included.

\newpage

\appendix

# Appendix {-}


# Additional data details

# Model details {#sec-model-details}

## Posterior predictive check

In @fig-ppcheckandposteriorvsprior-1 we implement a posterior predictive check. This shows...


## Diagnostics


\newpage


# References